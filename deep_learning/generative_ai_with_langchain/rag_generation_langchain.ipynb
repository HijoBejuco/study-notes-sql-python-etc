{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook's objective is to generate a simple RAG using LangChain. \n",
    "The guide for this notebook is [This](https://python.langchain.com/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use open source packages from HuggingFace like the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "# Ensure VertexAI credentials are configured.\n",
    "# Or configure here another LLM. \n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "model = ChatVertexAI(model=\"gemini-1.5-flash\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: indexing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading documents**. \n",
    "We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\n",
    "\n",
    "In this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text\n",
    "\n",
    "Documentations for Document loaders are: [Here](https://python.langchain.com/docs/how_to/#document-loaders), in the document loaders section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This library is used for parsing HTML content.\n",
    "    and scraping the content from the web page.\"\"\"\n",
    "import bs4\n",
    "\n",
    "\"\"\" This is a class used for loading documents from the web.\n",
    "    The next steps for this notebook could be how to load\n",
    "    doccuments from other sources like PDFs, Word documents, etc.\"\"\"\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "\"\"\"This checks that exactly one document was loaded. \n",
    "    If not, it raises an AssertionError.\"\"\"\n",
    "assert len(docs) == 1\n",
    "\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the first 500 characters of the document\n",
    "print(docs[0].page_content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting documents**\n",
    "Our loaded document is very long to fit into the context window of any model. \n",
    "\n",
    "To handle this, we'll split the document into chunks. \n",
    "\n",
    "This step is needed for the next steps where we apply **embedding** (*represent data (text) in a continuous vector space*) and **vector storage** (saving and retrieve vectors). This should help us retrieve only the most relevant parts of the blog post at run time.\n",
    "\n",
    "We will use ***RecursiveCharacterTextSplitter***, which splits the text using common separators such as new lines until each chunk is appropriate size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200, # Overlap characters between chunks\n",
    "    # if True, track index in original document, each \n",
    "    # chunk will have metadata indicating at which character\n",
    "    # it begins in the original text\n",
    "    add_start_index=True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_splits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Storing documents**\n",
    "Now, we need to index the text chunks so we can search over them at runtime. Here we embed the content of each split and insert these embeddings into a vector store. So we can, given an input query, use vector search to retrieve relevant documents. \n",
    "\n",
    "We can embed and store all the splits in a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've completed **the indexing** portion of the pipeline. We have generated a query-able vector store containing the chunked contents of the initial text. Given user questions, we should ideally be able to return the snippets of the blog post that answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
